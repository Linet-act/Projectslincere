# -*- coding: utf-8 -*-
"""Proyecto_Covid_Kmeans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qGUVhxhBMbz_X1ZWZkjZd8K98p7xHRIf

# Proyecto

Librería
"""

!python -m spacy download es

import pandas as pd
import missingno as msno
import numpy as np
import sklearn
import matplotlib.pyplot as plt
import seaborn as sns
import spacy
import collections
import re
import nltk
import en_core_web_sm
import os
import string
from bs4 import BeautifulSoup
from collections import Counter
from string import  punctuation
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import jaccard_score
from nltk.tokenize import regexp_tokenize, WhitespaceTokenizer
from nltk.stem import  WordNetLemmatizer
from nltk.corpus import  stopwords
from spacy.lang.es.stop_words import STOP_WORDS
from spacy.lang.es import Spanish
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
from nltk.tokenize import RegexpTokenizer  
from numpy import dot
from numpy.linalg import norm
from wordcloud import WordCloud, STOPWORDS
from spacy.matcher import Matcher

!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd content/

!mkdir proyecto

!ls -la

# Commented out IPython magic to ensure Python compatibility.
# %cd proyecto/

"""## Cargar Datos"""

from google.colab import files
uploaded = files.upload()

!ls -la

df= pd.read_csv("covid.csv", error_bad_lines=False, encoding="utf-8")
df.head()

"""# Estadísticas generales"""

df.info()

"""# Forma del Dataframe"""

df.shape

df.shape[0]

df.columns.values.tolist()

df.dtypes

df.isnull().any().any()

msno.matrix(df)

df.replace({' ': np.nan}, inplace=True)

msno.matrix(df)

msno.bar(df)

#Nuestros atributos a utilizar no estan vacios
df.columns[df.isnull().any()].tolist()

list(df['screenName'].unique())

"""# Seleccionar datos"""

data =df[['screenName','text' ]].copy()
data.head()

nltk.download('stopwords')

stopwords.fileids()

set(stopwords.words('spanish'))

#Eliminar hashtags y caracteres acentos
def clean_text(df, text_field):
    df[text_field] = df[text_field].str.lower()
    df[text_field] = df[text_field].astype(str).apply(lambda x: x.replace('á', 'a'))
    df[text_field] = df[text_field].apply(lambda x: x.replace('é', 'e'))
    df[text_field] = df[text_field].apply(lambda x: x.replace('í', 'i'))
    df[text_field] = df[text_field].apply(lambda x: x.replace('ó', 'o'))
    df[text_field] = df[text_field].apply(lambda x: x.replace('ú', 'u'))
    df[text_field] = df[text_field].apply(lambda elem: re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|t^rt|http.+?", "", str(elem)))
    return df

data['text']

data_clean1 = clean_text(data,'text')
data_clean1['text']

def clean_text_(df, text_field):
  df[text_field] = df[text_field].apply(lambda elem: re.sub(r"(u0\w+)|(u1\w+)|(u2\w+)|(u3\w+)|(u4\w+)|(u5\w+)|(u6\w+)|(u7\w+)|(u8\w+)|(u9\w+)", " ", str(elem)))
  return df

data_clean = clean_text_(data_clean1,'text')
data_clean['text']

"""# Tokenizar datos"""

tokenizer = RegexpTokenizer(r'\w+')

#Crear lista de puntuaciones
punctuations = string.punctuation
punctuations

set(stopwords.words('spanish'))
nlp = spacy.load("en_core_web_sm")
stop_words =stopwords.words('spanish') #spacy.lang.es.stop_words.STOP_WORDS
print(stop_words)

parser = Spanish()
parser

#Crear función Tokenizadora
exclude =['covid', 'covid19', 'ultimahora']
def tokenizer(sentence):
    nytokens=parser(sentence)
    nytokens = [ word.lemma_.lower().strip() if word.lemma_ != 'PRON' else word.lower_ for word in nytokens ]
    nytokens = [ word for word in nytokens if word  not in stop_words and word  not in punctuations and word not in exclude ]
    nytokens = ' '.join([i for i in nytokens])
    return nytokens

data_clean.text = data_clean.text.apply(tokenizer)
data_clean.text.head(40)

#Crear Corpus
corpus = []
dataframe = data_clean.text
news = dataframe.str.split().values.tolist()
corpus = [word.lower().strip() for i in news for word in i]
corpus[:10]

n_most = 20
most=Counter(corpus).most_common(n_most)
x, y = [], []
print (most)
for word, count in most:
  x.append(word)
  y.append(count)
sns.barplot(x=y, y=x)

word_cloud = WordCloud(
                    background_color='white',
                    stopwords=set(STOPWORDS),
                    max_words=50,
                    max_font_size=40,
                    scale=5,
                    random_state=1).generate(str(data_clean['text']))
fig = plt.figure(1, figsize=(10,10))
plt.axis('off')
fig.suptitle('Word Cloud for covid', fontsize=20)
fig.subplots_adjust(top=2.3)
plt.imshow(word_cloud)
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize

data_test = data_clean.text 

tf_idf_vectorizor =TfidfVectorizer(stop_words=stopwords.words("spanish"),
                                   min_df=0.001,   
                                    ngram_range=(2, 2), #bigram
                                    max_features=200000)

tf_idf = tf_idf_vectorizor.fit_transform(data_test)
tf_idf_norm = normalize(tf_idf)
tf_idf_array = tf_idf_norm.toarray()

print(tf_idf.shape)

tf_idf_vectorizor.get_feature_names()[:10]

"""# K-Means"""

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

sklearn_pca = PCA(n_components = 2)
Y_sklearn = sklearn_pca.fit_transform(tf_idf_array)

import plotly.express as px
fig = px.scatter(x=Y_sklearn[:, 0], y=Y_sklearn[:, 1])  
fig.update_traces(textposition='top center')
fig.update_layout(
    height=800,
    title_text='tweets clustering'
)

fig.show()

number_clusters = range(1, 10)

kmeans = [KMeans(n_clusters=i, max_iter = 600) for i in number_clusters]
kmeans

score = [kmeans[i].fit(Y_sklearn).score(Y_sklearn) for i in range(len(kmeans))]
score

plt.plot(number_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Method')
plt.show()

kmeans = KMeans(n_clusters=5, max_iter=600, algorithm = 'auto')
fitted = kmeans.fit(Y_sklearn)
prediction = kmeans.predict(Y_sklearn)

def get_top_features_cluster(tf_idf_array, prediction, n_feats):
    labels = np.unique(prediction)
    dfs = []
    for label in labels:
        id_temp = np.where(prediction==label) # indices for each cluster, the most cluster is...
        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster
        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores
        features = tf_idf_vectorizor.get_feature_names()
        best_features = [(features[i], x_means[i]) for i in sorted_means]
        df = pd.DataFrame(best_features, columns = ['features', 'score'])
        dfs.append(df)
    return dfs

dfs = get_top_features_cluster(tf_idf_array, prediction, 5)

len(dfs)

", ".join(dfs[0].features.unique())

#set up colors per clusters using a dict
cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}

#set up cluster names using a dict
cluster_names = {0: ", ".join(dfs[0].features.unique()), 
                 1: ", ".join(dfs[1].features.unique()), 
                 2: ", ".join(dfs[2].features.unique()),
                 3: ", ".join(dfs[3].features.unique()),
                 4: ", ".join(dfs[4].features.unique())}

cluster_names

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 
df_final = pd.DataFrame(dict(x=Y_sklearn[:, 0], 
                          y=Y_sklearn[:, 1], 
                          label=prediction, 
                          name=data.screenName)) 

groups = df_final.groupby('label')
groups.count()

df_final.head()

df_final.label.value_counts()

def deEmojify(inputString):
    return inputString.encode('ascii', 'ignore').decode('ascii')

df.columns

plt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1], c= kmeans.labels_, s=30, cmap='viridis')
centers = fitted.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1],c='red', s=150, alpha=0.7, label = 'centroides')
plt.title("Clusters de Tweets sobre Covid-19 en CDMX")
plt.legend()
plt.show();

import plotly.express as px
df_final["label"]= df_final["label"].astype(str)

df_final["screenName"] = df["screenName"]

df_final["text"] = df["text"]

fig = px.scatter(df_final.sample(100).dropna(), x="x", y="y", 
                 color="label",  # label, user_location
                 hover_data = ["screenName", "text"],
                 #size_max=60,
                 text="screenName")

fig.update_traces(textposition='top center')

fig.update_layout(
    height=800,
    title_text='tweets clustering'
)

fig.show()

"""# Validacion K-Fold"""

from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score
# create dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)
# prepare the cross-validation procedure
cv = RepeatedKFold(n_splits=10, n_repeats=100, random_state=1)
# create model
model = KMeans()
# evaluate model
scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))

# compare the number of repeats for repeated k-fold cross-validation
from scipy.stats import sem
from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score
from matplotlib import pyplot
 
# evaluate a model with a given number of repeats
def evaluate_model(X, y, repeats):
	# prepare the cross-validation procedure
	cv = RepeatedKFold(n_splits=10, n_repeats=repeats, random_state=1)
	# create model
	model = KMeans ()
	# evaluate model
	scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
	return scores
 
# create dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)
# configurations to test
repeats = range(1,16)
results = list()
for r in repeats:
	# evaluate using a given number of repeats
	scores = evaluate_model(X, y, r)
	# summarize
	print('>%d mean=%.4f se=%.3f' % (r, mean(scores), sem(scores)))
	# store
	results.append(scores)
# plot the results
pyplot.boxplot(results, labels=[str(r) for r in repeats], showmeans=True)
pyplot.show()